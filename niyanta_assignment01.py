# -*- coding: utf-8 -*-
"""Niyanta_301224927.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13tjZqP0gK43UMmnWIwcKD_XCSEDu_DQ4
"""

#QUESTION 1
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

#1.Retrieve and load the mnist_784 dataset of 70,000 instances. [5 points]
from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784',version=1)

print(mnist.keys())

X,Y = mnist['data'],mnist['target']
print(X.shape, Y.shape)

X = pd.DataFrame(X).to_numpy()
Y = pd.DataFrame(X).to_numpy()

#2.Display each digit. [5 points]
def plot_digit(data, target, index):
    some_digit = data[index]
    print(target[index])
    print(some_digit)
    print(type(some_digit))
    print(type(Y))
    some_digit_image = some_digit.reshape(28, 28)
    plt.imshow(some_digit_image, cmap=mpl.cm.binary)
    plt.axis("off")
    plt.show()

plot_digit(X,Y,1)#0
plot_digit(X,Y,3)#1
plot_digit(X,Y,5)#2
plot_digit(X,Y,7)#3
plot_digit(X,Y,9)#4
plot_digit(X,Y,0)#5
plot_digit(X,Y,11)#6
plot_digit(X,Y,15)#7
plot_digit(X,Y,17)#8
plot_digit(X,Y,4)#9

print(Y)

#3.Use PCA to retrieve the principal component and output their explained variance ratio. [5 points]
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit(X)
scaled_data=scaler.transform(X)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(scaled_data)
X_pca = pca.transform(scaled_data)

print(pca.explained_variance_ratio_)

#4.Plot the projections of the principal component onto a 2D hyperplane. [5 points]

ca = PCA(n_components=2)
X2D = pca.fit_transform(X)
print('explained variance ratio:')
print(pca.explained_variance_ratio_)
holder = [0 for i in range(X2D.shape[0])]
plt.title('the projection of first component')
plt.scatter(X2D[:, 0], holder)
plt.show()
plt.title('the projection of second component')
plt.scatter(X2D[:, 1], holder)
plt.show()

#5.Use Incremental PCA to reduce the dimensionality of the MNIST dataset down to 154 dimensions. [10 points]
from sklearn.decomposition import  IncrementalPCA

ipca = IncrementalPCA(n_components=154)
for batch in np.array_split(scaled_data, 100):
	ipca.partial_fit(batch)
    
    
X_reduced = pca.transform(X)
X_recovered = pd.DataFrame(pca.inverse_transform(X_reduced))
X_pca.shape

#6.Display the original and compressed digits from (5). [5 points]

import seaborn as sns

fig, axarr = plt.subplots(10, 2, figsize=(12, 40))
sns.heatmap(X[1].reshape(28, 28), ax=axarr[0][0], cmap='gray_r')
sns.heatmap(ipca.components_[0, :].reshape(28, 28), ax=axarr[0][1], cmap='gray_r')


sns.heatmap(X[3].reshape(28, 28), ax=axarr[1][0], cmap='gray_r')
sns.heatmap(ipca.components_[1, :].reshape(28, 28), ax=axarr[1][1], cmap='gray_r')

sns.heatmap(X[5].reshape(28, 28), ax=axarr[2][0], cmap='gray_r')
sns.heatmap(ipca.components_[2, :].reshape(28, 28), ax=axarr[2][1], cmap='gray_r')

sns.heatmap(X[7].reshape(28, 28), ax=axarr[3][0], cmap='gray_r')
sns.heatmap(ipca.components_[3, :].reshape(28, 28), ax=axarr[3][1], cmap='gray_r')

sns.heatmap(X[9].reshape(28, 28), ax=axarr[4][0], cmap='gray_r')
sns.heatmap(ipca.components_[4, :].reshape(28, 28), ax=axarr[4][1], cmap='gray_r')

sns.heatmap(X[0].reshape(28, 28), ax=axarr[5][0], cmap='gray_r')
sns.heatmap(ipca.components_[5, :].reshape(28, 28), ax=axarr[5][1], cmap='gray_r')

sns.heatmap(X[11].reshape(28, 28), ax=axarr[6][0], cmap='gray_r')
sns.heatmap(ipca.components_[6, :].reshape(28, 28), ax=axarr[6][1], cmap='gray_r')

sns.heatmap(X[15].reshape(28, 28), ax=axarr[7][0], cmap='gray_r')
sns.heatmap(ipca.components_[7, :].reshape(28, 28), ax=axarr[7][1], cmap='gray_r')

sns.heatmap(X[17].reshape(28, 28), ax=axarr[8][0], cmap='gray_r')
sns.heatmap(ipca.components_[8, :].reshape(28, 28), ax=axarr[8][1], cmap='gray_r')

sns.heatmap(X[4].reshape(28, 28), ax=axarr[9][0], cmap='gray_r')
sns.heatmap(ipca.components_[9, :].reshape(28, 28), ax=axarr[9][1], cmap='gray_r')

#7.Show the Demonstration in Lab or Create a video discussing the code and result for each question.
#8.Write an analysis report discussing the challenges you confronted and solutions to overcome them, if applicable [15 points]

#--------------------------------------------------------------------------------------------------------------------------------

#QUESTION - 2

import numpy as np, matplotlib.pyplot as plt
from sklearn.datasets import make_swiss_roll
from sklearn.decomposition import KernelPCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

#1.Generate Swiss roll dataset. [5 points]
from sklearn.datasets import make_swiss_roll
X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)

#2.Plot the resulting generated Swiss roll dataset. [2 points]

axes = [-11.5, 14, -2, 23, -12, 15]

fig = plt.figure(figsize=(6, 5))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)
ax.view_init(10, -70)
ax.set_xlabel("$x_1$", fontsize=18)
ax.set_ylabel("$x_2$", fontsize=18)
ax.set_zlabel("$x_3$", fontsize=18)
ax.set_xlim(axes[0:2])
ax.set_ylim(axes[2:4])
ax.set_zlim(axes[4:6])

#save_fig("swiss_roll_plot")
plt.show()

#3.Use Kernel PCA (kPCA) with linear kernel (2 points), a RBF kernel (2 points), and a sigmoid kernel (2 points). [6 points]
#4.Plot the kPCA results of applying the linear kernel (2 points), a RBF kernel (2 points), and a sigmoid kernel (2 points) from (3). Explain and compare the results in your analysis report [6 points]

from sklearn.decomposition import KernelPCA

lin_pca = KernelPCA(n_components = 2, kernel="linear", fit_inverse_transform=True)
rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433, fit_inverse_transform=True)
sig_pca = KernelPCA(n_components = 2, kernel="sigmoid", gamma=0.001, coef0=1, fit_inverse_transform=True)

y = t > 6.9

plt.figure(figsize=(11, 4))
for subplot, pca, title in ((131, lin_pca, "Linear kernel"), (132, rbf_pca, "RBF kernel, $\gamma=0.04$"), (133, sig_pca, "Sigmoid kernel, $\gamma=10^{-3}, r=1$")):
    X_reduced = pca.fit_transform(X)
    if subplot == 132:
        X_reduced_rbf = X_reduced
    
    plt.subplot(subplot)
    plt.title(title, fontsize=14)
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
    plt.xlabel("$z_1$", fontsize=18)
    if subplot == 131:
        plt.ylabel("$z_2$", fontsize=18, rotation=0)
    plt.grid(True)

#save_fig("kernel_pca_plot")
plt.show()

t = np.where(t <= t.mean(), 0, 1)
pipeline = Pipeline([('pca', KernelPCA(n_components=2)), ('model', LogisticRegression())])
param_grid = {'pca__kernel': ['linear', 'rbf', 'sigmoid'], 'pca__gamma': np.arange(0, 1, .02), 'model__max_iter': [1000, 2000, 4000, 5000]}
grid_search = GridSearchCV(pipeline, param_grid, scoring='accuracy', refit=True)
grid_search.fit(X, t)



print('best estimator:')
print(grid_search.best_estimator_)
print('best score:')
print(grid_search.best_score_)

